{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKTDObzfLQ1j"
   },
   "source": [
    "# Procesamiento de microdatos de la Encuesta de Presupuestos Familiares\n",
    "### Procedimiento para la transformacióón de los archivos de Microdatos descargados de la web del INE para su conversión en archivos CSV\n",
    "Nota: Con posterioridad, se pueden subir estos archivos a un servidor cloud para convertirlo en una base de datos analítica que permita explorar los datos mediante SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuhEUPiTOZFH"
   },
   "source": [
    "#### Explicamos paso a paso la ejecutión del procesado de archivos:\n",
    "Procedimiento:\n",
    "1. Conectamos Drive (Paso 1) con el archivo y carpeta compartida \"aca_folder\"\n",
    "2. Introducimos los archivos ZIP previamente descargados desde la web del INE en la carpeta Drive \"zip_microdatos\"\n",
    "3. Ejecutamos todas las celdas que hay a continuación hasta el final\n",
    "4. Encontraremos los archivos CSV en la carpeta Drive \"epf_folder\", separados por hogar y gastos. \n",
    "\n",
    "Dentro de cada ZIP descargado de la web del INE, en el apartado de datos habrá ficheros relativos a Hogares, Gastos y Miembros. Para este proceso de transformación, no consideramos los datos de Miembros ya que en el análisis previo al presente no se consideró para su estudio.\n",
    "\n",
    "Al tratarse de dos archivos relacionados, se ha elaborado un directorio de carpetas separando todos los archivos relativos a Hogar y Gastos. Para poder relacionarlos entre sí y facilitar su indexado, se elabora un ID combinando la fecha de la encuesta con la variable NUMERO, que es el id anual que proporciona el INE.\n",
    "\n",
    "\n",
    "**Paso 1:** Ejecutamos la celda haciendo click en el síímbolo de play (▶️) que aparece a la izquierda (colocar ratón por encima para visualizarlo).\n",
    "\n",
    "Conectamos con nuestro Drive para acceder el conjunto de carpetas que alojan nuestros datos y donde, después, se volcarán los resultados. Aparecerá un enlace. Hay que hacer click, aceptar los términos y copiar la clave para pegarla en la casilla.\n",
    "Una vez hecho, pulsar Enter.\n",
    "\n",
    "**Paso 2:** Haciendo click en la segunda casilla, pulsar el comando **Ctrl + F9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Ukcu_J7eDT79",
    "outputId": "3ae12dae-452b-42f3-97fa-c5e0212d0f7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cstTBnDiE49Z"
   },
   "outputs": [],
   "source": [
    "import zipfile, os, shutil, json\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#Rutas y directorios\n",
    "\n",
    "ascii_path = '/content/drive/My Drive/aca_folder/ascii_docs'\n",
    "zips_path = '/content/drive/My Drive/aca_folder/zip_microdatos'\n",
    "zips_2_path = '/content/drive/My Drive/aca_folder/zip_microdatos/second_zip_files'\n",
    "datos_zip = os.path.join(zips_path, 'datos')\n",
    "\n",
    "destination_path = os.path.join('/content/drive/My Drive/aca_folder', 'epf_folder')\n",
    "directory_gastos = os.path.join(destination_path,'tablas_gastos')\n",
    "directory_hogar = os.path.join(destination_path,'tablas_hogar')\n",
    "\n",
    "if not os.path.exists(destination_path):\n",
    "    try:\n",
    "        os.makedirs(destination_path)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "if not os.path.exists(directory_gastos) and not os.path.exists(directory_hogar):\n",
    "    try:\n",
    "        os.makedirs(directory_gastos)\n",
    "        os.makedirs(directory_hogar)\n",
    "        os.makedirs(datos_zip)\n",
    "        os.makedirs(zips_2_path)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "if len(os.listdir(zips_2_path)) != 0:\n",
    "    try:\n",
    "        for filename in os.listdir(zips_2_path):\n",
    "            os.remove(filename) \n",
    "    except OSError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gSLAgXPlFYXw"
   },
   "outputs": [],
   "source": [
    "#Definimos las funciones a emplear para extraer documentos ASCII de los ficheros comprimidos que nos descargamos del INE\n",
    "\n",
    "def unzip(source_filename, dest_dir):\n",
    "    with zipfile.ZipFile(source_filename) as zf:\n",
    "        for member in zf.infolist():\n",
    "            words = member.filename.split('/')\n",
    "            path = dest_dir\n",
    "            for word in words[:-1]:\n",
    "                while True:\n",
    "                    drive, word = os.path.splitdrive(word)\n",
    "                    head, word = os.path.split(word)\n",
    "                    if not drive:\n",
    "                        break\n",
    "                if word in (os.curdir, os.pardir, ''):\n",
    "                    continue\n",
    "                path = os.path.join(path, word)\n",
    "            zf.extract(member, path)\n",
    "\n",
    "def safe_unzip(zip_file, extractpath='.'):\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zf:\n",
    "        for member in zf.infolist():\n",
    "            abspath = os.path.abspath(os.path.join(extractpath, member.filename))\n",
    "            if abspath.startswith(os.path.abspath(extractpath)) and abspath.endswith(\".zip\"):\n",
    "                zf.extract(member, extractpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6u0j762nDo2l"
   },
   "outputs": [],
   "source": [
    "# Archivos auxiliares\n",
    "\n",
    "# Diccionario con la información de nombre de columnas y posición dentro del archivo ASCII FWF (Fixed Width File). De este tipo de archivo, \n",
    "# extraemos la columna y valores correspondientes en base a unas \"coordenadas\" que facilitan en un fichero para SPSS que viene descargado en el fichero disreg.zip\n",
    "\n",
    "# El diccinario se ha realizado ad hoc para los datos de la EPF.\n",
    "# To do: Implantar algoritmo que lea ese archivo con las coordoneadas SPSS y las añada al diccionario. \n",
    "# Hay que tener en cuenta que se suma 1 al inicio de cada tupla[0] y comprobar que la tupla[1] encaje con el inicio de la siguiente.\n",
    "\n",
    "with open('/content/drive/My Drive/aca_folder/epf_fwf_info.json', 'r') as f:\n",
    "    epf_fwf_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "aafFKkLbHVGB",
    "outputId": "01ddda25-a893-4447-e462-ba9f262cd975"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datos_epf2006.zip', 'disreg_epf2006.zip', 'datos_epf2007.zip', 'disreg_epf2007.zip', 'datos_epf2008.zip', 'disreg_epf2008.zip', 'datos_epf2009.zip', 'disreg_epf2009.zip', 'datos_epf2010.zip', 'disreg_epf2010.zip', 'epf_2011_datos.zip', 'disreg_epf2011.zip', 'epf_2012_datos.zip', 'disreg_epf2012.zip', 'epf_2013_datos.zip', 'disreg_epf2013.zip', 'epf_2014_datos.zip', 'disreg_epf2014.zip', 'datos_epf2015.zip', 'disreg_epf2015.zip', 'Aviso uso correcto microdatos 2016.txt', 'datos_epf2016.zip', 'disreg_epf2016.zip', 'datos_epf2017.zip', 'disreg_epf2017.zip', 'datos_epf2018.zip', 'disreg_epf2018.zip', 'datos_2019']\n"
     ]
    }
   ],
   "source": [
    "# Extraemos los archivos zips de los ficheros descargados del INE\n",
    "# Los archivos del INE se estructuran en la siguiente arquitectura:\n",
    "# --datos_2018.zip\n",
    "#   |__datos_epf2018.zip\n",
    "#   |           |_______Fichero de usuario de gastos a2018\n",
    "#   |           |_______Fichero de usuario de hogar a2018\n",
    "#   |           |_______Fichero de usuario de miembros a2018\n",
    "#   |\n",
    "#   |__disreg_epf2018.zip\n",
    "#               |______Inputs 2018.txt\n",
    "#               |______Inputs SPSS 2018.txt\n",
    "#\n",
    "########\n",
    "# NOTA #\n",
    "########\n",
    "#\n",
    "# En 2019 el formato cambia para tener esta estructura:\n",
    "#--datos_2019.zip\n",
    "#  |___datos_2019\n",
    "#      |__________Fichero de usuario de gastos a2019\n",
    "#      |__________Fichero de usuario de gastos a2019\n",
    "#      |__________Fichero de usuario de gastos a2019\n",
    "#      |__________Inputs 2018.txt\n",
    "#      |__________Inputs SPSS 2018.txt\n",
    "\n",
    "\n",
    "try:\n",
    "    for zips in os.listdir(zips_path):\n",
    "        unzip(os.path.join(zips_path, zips), zips_2_path)\n",
    "except:\n",
    "    print(\"Algo ha ido mal con la extracción del primer nivel\")\n",
    " # Extraes los archivos zip de segundo nivel pero exclusivamente los que contienen los datos (x2 porque los ficheros de 2011-2014) están doblemente comprimidos\n",
    "try:\n",
    "    for zips in os.listdir(zips_2_path):\n",
    "        for date in range(2006, int(today.strftime(\"%Y\"))):  \n",
    "            if zips.endswith(f'epf{date}.zip') and not zips.startswith('disreg'):\n",
    "                unzip(os.path.join(zips_2_path, zips), datos_zip)\n",
    "\n",
    "    for zips in os.listdir(datos_zip):\n",
    "        for date in range(2006, int(today.strftime(\"%Y\"))):  \n",
    "            if zips.endswith(f'epf{date}.zip') and not zips.startswith('disreg'):\n",
    "                unzip(os.path.join(datos_zip, zips), datos_zip)\n",
    "except:\n",
    "    print(\"Algo ha salido mal. Revisar archivos de los directorios operativos dentro de zip_microdatos\")\n",
    "\n",
    "#Indexas todo el directorio hacia abajo para extraer los archivos descomprimidos y añadirles su root\n",
    "# de esta forma tiene la ruta terminada para poder moverlos a ascii_docs\n",
    "try:\n",
    "    ascii_files = set([os.path.join(root, i) for root, dirs, files in os.walk(zips_path) for i in files if i.startswith(\"Fichero de usuario de hogar\") or i.startswith(\"Fichero de usuario de gastos\")])\n",
    "    for i in ascii_files:\n",
    "        if i.split('/')[-1] not in os.listdir(ascii_path):\n",
    "            shutil.move(i, ascii_path)\n",
    "except Exception as e:\n",
    "    print(\"La recuperacióón de los ficheros ASCII ha ido mal. Revisar listado de archivos recuperados o funcióón shutil.move\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-Dtsa4OzbFz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyfTHUEEza5R"
   },
   "outputs": [],
   "source": [
    "filelist = os.listdir(ascii_path)\n",
    "\n",
    "try:\n",
    "    for file in filelist:\n",
    "        if \"gastos\" in file:\n",
    "            file_date = file.split(\" \")[-1]\n",
    "            date = int(re.split('(\\d+)',file_date)[1])\n",
    "            path = ascii_path\n",
    "          # For all listed files in directory, we extract name and date, then we apply transformation of features\n",
    "          # according to specefications detailed by INE dcumentation\n",
    "\n",
    "            if date < 2016:\n",
    "                file_name = os.path.join(path, file)\n",
    "                df = pd.read_fwf(os.path.join(path, file), names=epf_fwf_info[\"col_names_gastos_2006\"], header=None, colspecs=epf_fwf_info[\"colspecs_gastos_2006\"])\n",
    "                df[\"ID\"] = df[\"ANOENC\"].astype(int).astype(str) + df[\"NUMERO\"].astype(int).astype(str) # We also create a custom ID so we can merge all file in a single table after on the DataBase\n",
    "                df[\"ANOENC\"] = df[\"ANOENC\"].astype(int)\n",
    "                df[\"NUMERO\"] = df[\"NUMERO\"].astype(int)\n",
    "\n",
    "              # Before dumping the csv, we must standarize the whole dataframe so we avoid issues when uploading to the\n",
    "              # database server\n",
    "                columnas_extra = [i for i in set(epf_fwf_info[\"col_names_gastos_2006\"]+epf_fwf_info[\"col_names_gastos_2016\"]) if i not in set(list(df.columns))]\n",
    "                df_aux = pd.concat([df, pd.DataFrame(columns=columnas_extra)], sort=True)\n",
    "\n",
    "                df_aux.to_csv(os.path.join(directory_gastos,f'gastos_epf_{date}.csv'), index=False)\n",
    "            else:\n",
    "                file_name = os.path.join(path, file)\n",
    "                df = pd.read_fwf(os.path.join(path, file), names=epf_fwf_info[\"col_names_gastos_2016\"], header=None, colspecs=epf_fwf_info[\"colspecs_gastos_2016\"])\n",
    "                df[\"ID\"] = df[\"ANOENC\"].astype(int).astype(str) + df[\"NUMERO\"].astype(int).astype(str)\n",
    "                df[\"ANOENC\"] = df[\"ANOENC\"].astype(int)\n",
    "                df[\"NUMERO\"] = df[\"NUMERO\"].astype(int)\n",
    "                columnas_extra = [i for i in set(epf_fwf_info[\"col_names_gastos_2006\"]+epf_fwf_info[\"col_names_gastos_2016\"]) if i not in set(list(df.columns))]\n",
    "                df_aux = pd.concat([df, pd.DataFrame(columns=columnas_extra)], sort=True)\n",
    "\n",
    "                df_aux.to_csv(os.path.join(directory_gastos,f'gastos_epf_{date}.csv'), index=False)\n",
    "        if \"hogar\" in file:\n",
    "            file_date = file.split(\" \")[-1]\n",
    "            date = int(re.split('(\\d+)',file_date)[1])\n",
    "          # For all listed files in directory, we extract name and date, then we apply transformation of features\n",
    "          # according to specefications detailed by INE dcumentation\n",
    "\n",
    "            if date < 2016:\n",
    "                file_name = os.path.join(path, file)\n",
    "                df = pd.read_fwf(os.path.join(path, file), names=epf_fwf_info[\"col_names_hogar_2006\"], header=None, colspecs=epf_fwf_info[\"colspecs_hogar_2006\"])\n",
    "                df[\"ID\"] = df[\"ANOENC\"].astype(int).astype(str) + df[\"NUMERO\"].astype(int).astype(str)\n",
    "                df[\"ANOENC\"] = df[\"ANOENC\"].astype(int)\n",
    "                df[\"NUMERO\"] = df[\"NUMERO\"].astype(int)\n",
    "\n",
    "              # Before dumping the csv, we must standarize the whole dataframe so we avoid issues when uploading to the\n",
    "              # database server\n",
    "                columnas_extra = [i for i in set(epf_fwf_info[\"col_names_hogar_2006\"]+epf_fwf_info[\"col_names_hogar_2016\"]) if i not in set(list(df.columns))]\n",
    "                df_aux = pd.concat([df, pd.DataFrame(columns=columnas_extra)], sort=True)\n",
    "\n",
    "                df_aux.to_csv(os.path.join(directory_hogar,f'hogar_epf_{date}.csv'), index=False)\n",
    "            else:\n",
    "                file_name = os.path.join(path, file)\n",
    "                df = pd.read_fwf(os.path.join(path, file), names=epf_fwf_info[\"col_names_hogar_2016\"], header=None, colspecs=epf_fwf_info[\"colspecs_hogar_2016\"])\n",
    "                df[\"ID\"] = df[\"ANOENC\"].astype(int).astype(str) + df[\"NUMERO\"].astype(int).astype(str)\n",
    "                df[\"ANOENC\"] = df[\"ANOENC\"].astype(int)\n",
    "                df[\"NUMERO\"] = df[\"NUMERO\"].astype(int)\n",
    "                columnas_extra = [i for i in set(epf_fwf_info[\"col_names_hogar_2006\"]+epf_fwf_info[\"col_names_hogar_2016\"]) if i not in set(list(df.columns))]\n",
    "                df_aux = pd.concat([df, pd.DataFrame(columns=columnas_extra)], sort=True)        \n",
    "\n",
    "                df_aux.to_csv(os.path.join(directory_hogar,f'hogar_epf_{date}.csv'), index=False)\n",
    "except Exception as e:\n",
    "    print(\"Algo ha ido mal durante la transformacióón de ficheros (bucle dentro de la carpeta ASCII). Buena suerte\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WZgeFRvzava"
   },
   "outputs": [],
   "source": [
    "print(\"El proceso ha finalizado con éxito. Los archivos se encuentran en formato CSV en la carpeta epf_folder/hogar y epf_folder/gastos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1KfUmYy0zaip"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "epf_microdata_transform v0.0.1",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
